cephClusterSpec:
  dashboard:
    enable: true
  mgr:
    # When higher availability of the mgr is needed, increase the count to 2.
    # In that case, one mgr will be active and one in standby. When Ceph updates which
    # mgr is active, Rook will update the mgr services to match the active mgr.
    count: 2
  storage: # cluster level storage configuration and selection
    useAllNodes: false
    useAllDevices: false
  deviceFilter:
  # config:
  #   crushRoot: "custom-root" # specify a non-default root label for the CRUSH map
  #   metadataDevice: "md0" # specify a non-rotational storage so ceph-volume will use it as block db device of bluestore.
  #   databaseSizeMB: "1024" # uncomment if the disks are smaller than 100 GB
  #   journalSizeMB: "1024"  # uncomment if the disks are 20 GB or smaller
  #   osdsPerDevice: "1" # this value can be overridden at the node or device level
  #   encryptedDevice: "true" # the default value for this option is "false"
  # # Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false. Then, only the named
  # # nodes below will be used as storage resources. Each node's 'name' field should match their 'kubernetes.io/hostname' label.
  nodes:
    - name: "10.150.10.30"
      devices: # specific devices to use for storage can be specified for each node
        - name: "sdb"
      config: # configuration can be specified at the node level which overrides the cluster level config
    - name: "10.150.10.31"
      devices: # specific devices to use for storage can be specified for each node
        - name: "sdb"
      config: # configuration can be specified at the node level which overrides the cluster level config
    - name: "10.150.10.32"
      devices: # specific devices to use for storage can be specified for each node
        - name: "sdb"
      config: # configuration can be specified at the node level which overrides the cluster level config